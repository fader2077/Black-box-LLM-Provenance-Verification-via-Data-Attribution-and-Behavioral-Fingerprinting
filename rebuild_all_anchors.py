#!/usr/bin/env python3
"""
å®Œå…¨é‡å»ºæ‰€æœ‰é”šç‚¹æŒ‡çº¹ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ¢é’ˆæ•°é‡
ç¡®ä¿æ•°æ®è´¨é‡å’Œç»´åº¦ä¸€è‡´æ€§
"""

import sys
import json
from pathlib import Path
from typing import Dict, List, Optional
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.probes import build_all_probes
from src.fingerprint import extract_fingerprint
from src.utils.unified_loader import load_model

# é”šç‚¹æ¨¡å‹æ˜ å°„
ANCHOR_MODEL_MAPPING = {
    "qwen2.5:0.5b": {
        "hf_model": "Qwen/Qwen2.5-0.5B",
        "ollama_model": "qwen2.5:7b",
        "engine": "transformers",
        "metadata": {
            "name": "qwen2.5:0.5b",
            "source": "china",
            "category": "qwen",
            "vendor": "Alibaba",
            "base_model": "Qwen2.5-0.5B",
            "description": "é˜¿é‡Œå·´å·´ Qwen ç³»åˆ—æ¨¡å‹ï¼ˆ0.5Bå‚æ•°ï¼‰"
        }
    },
    "gpt2": {
        "hf_model": "gpt2",
        "ollama_model": None,
        "engine": "transformers",
        "metadata": {
            "name": "gpt2",
            "source": "openai",
            "category": "gpt",
            "vendor": "OpenAI",
            "base_model": "GPT-2",
            "description": "OpenAI GPT-2 æ¨¡å‹ï¼ˆ124Må‚æ•°ï¼‰"
        }
    },
    "gpt2-medium": {
        "hf_model": "gpt2-medium",
        "ollama_model": None,
        "engine": "transformers",
        "metadata": {
            "name": "gpt2-medium",
            "source": "openai",
            "category": "gpt",
            "vendor": "OpenAI",
            "base_model": "GPT-2-Medium",
            "description": "OpenAI GPT-2 Medium æ¨¡å‹ï¼ˆ355Må‚æ•°ï¼‰"
        }
    },
    "yi:6b": {
        "hf_model": "01-ai/Yi-6B",
        "ollama_model": "yi:6b",
        "engine": "transformers",
        "metadata": {
            "name": "yi:6b",
            "source": "china",
            "category": "yi",
            "vendor": "01.AI",
            "base_model": "Yi-6B",
            "description": "é›¶ä¸€ä¸‡ç‰© Yi ç³»åˆ—æ¨¡å‹ï¼ˆ6Bå‚æ•°ï¼‰"
        }
    },
    "deepseek-r1:7b": {
        "hf_model": None,  # éœ€è¦æˆæƒ
        "ollama_model": "deepseek-r1:7b",
        "engine": "ollama",
        "metadata": {
            "name": "deepseek-r1:7b",
            "source": "china",
            "category": "deepseek",
            "vendor": "DeepSeek",
            "base_model": "DeepSeek-R1-7B",
            "description": "DeepSeek-R1 ç³»åˆ—ï¼ˆ7Bå‚æ•°ï¼‰"
        }
    },
    "llama3.2:1b": {
        "hf_model": "meta-llama/Llama-3.2-1B",
        "ollama_model": "llama3.2:1b",
        "engine": "transformers",  # ä¼˜å…ˆå°è¯• transformers
        "metadata": {
            "name": "llama3.2:1b",
            "source": "meta",
            "category": "llama",
            "vendor": "Meta",
            "base_model": "Llama-3.2-1B",
            "description": "Meta Llama 3.2 ç³»åˆ—ï¼ˆ1Bå‚æ•°ï¼‰"
        }
    },
    "gemma2:2b": {
        "hf_model": "google/gemma-2b",
        "ollama_model": "gemma2:2b",
        "engine": "transformers",
        "metadata": {
            "name": "gemma2:2b",
            "source": "google",
            "category": "gemma",
            "vendor": "Google",
            "base_model": "Gemma-2B",
            "description": "Google Gemma ç³»åˆ—ï¼ˆ2Bå‚æ•°ï¼‰"
        }
    },
}


def extract_single_anchor(
    anchor_name: str,
    config: Dict,
    probes: List,
    output_dir: Path,
    force: bool = False
) -> Optional[Dict]:
    """æå–å•ä¸ªé”šç‚¹æ¨¡å‹çš„æŒ‡çº¹"""
    
    output_file = output_dir / f"{anchor_name.replace(':', '_').replace('-', '_')}_fingerprint.json"
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if output_file.exists() and not force:
        logger.info(f"â­ï¸  è·³è¿‡ {anchor_name}ï¼ˆå·²å­˜åœ¨ï¼Œä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°æå–ï¼‰")
        return None
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“ æå–é”šç‚¹: {anchor_name}")
    logger.info(f"{'='*60}")
    
    engine = config["engine"]
    model_id = config["hf_model"] if engine == "transformers" else config["ollama_model"]
    
    if model_id is None:
        logger.warning(f"âš ï¸  {anchor_name} æ— å¯ç”¨æ¨¡å‹ï¼Œè·³è¿‡")
        return None
    
    logger.info(f"  å¼•æ“: {engine}")
    logger.info(f"  æ¨¡å‹: {model_id}")
    
    try:
        # åŠ è½½æ¨¡å‹
        logger.info(f"â³ åŠ è½½æ¨¡å‹...")
        model = load_model(model_id, engine=engine)
        
        # æå–æŒ‡çº¹
        logger.info(f"â³ æå–æŒ‡çº¹ï¼ˆ{len(probes)} ä¸ªæ¢é’ˆï¼‰...")
        fingerprint = extract_fingerprint(
            model_interface=model,
            probes=probes,
            include_logit=True,
            include_refusal=False  # æš‚æ—¶ç¦ç”¨æ‹’ç»æ£€æµ‹ï¼ŒåŠ å¿«é€Ÿåº¦
        )
        
        # éªŒè¯æŒ‡çº¹è´¨é‡
        fp_vector = fingerprint["logit_fingerprint"]["vector"]
        fp_dim = len(fp_vector)
        
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯ 0
        if all(v == 0.0 for v in fp_vector):
            logger.error(f"âŒ {anchor_name} æŒ‡çº¹å…¨æ˜¯ 0ï¼Œæå–å¤±è´¥ï¼")
            return None
        
        # æ£€æŸ¥ç»´åº¦
        if fp_dim < 10:
            logger.warning(f"âš ï¸  {anchor_name} æŒ‡çº¹ç»´åº¦è¿‡å°: {fp_dim}")
        
        logger.success(f"âœ… {anchor_name} æŒ‡çº¹æå–æˆåŠŸ")
        logger.info(f"  ç»´åº¦: {fp_dim}")
        logger.info(f"  å‡å€¼: {fingerprint['logit_fingerprint']['stats']['mean']:.4f}")
        logger.info(f"  æ ‡å‡†å·®: {fingerprint['logit_fingerprint']['stats']['std']:.4f}")
        
        # ä¿å­˜æŒ‡çº¹
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(fingerprint, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")
        
        return {
            "anchor_name": anchor_name,
            "config": config,
            "fingerprint_file": str(output_file.relative_to(project_root)),
            "dimension": fp_dim,
            "success": True
        }
        
    except Exception as e:
        logger.error(f"âŒ {anchor_name} æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    import argparse
    parser = argparse.ArgumentParser(description="é‡æ–°æå–æ‰€æœ‰é”šç‚¹æŒ‡çº¹")
    parser.add_argument("--num-probes", type=int, default=150,
                        help="æ¯ä¸ªç±»åˆ«çš„æ¢é’ˆæ•°é‡ï¼ˆæ€»æ•°=3*num_probesï¼‰")
    parser.add_argument("--force", action="store_true",
                        help="å¼ºåˆ¶é‡æ–°æå–ï¼ˆå³ä½¿æ–‡ä»¶å·²å­˜åœ¨ï¼‰")
    parser.add_argument("--anchors", nargs="+",
                        help="åªæå–æŒ‡å®šçš„é”šç‚¹ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰")
    args = parser.parse_args()
    
    logger.info("="*80)
    logger.info("ğŸ”„ é‡æ–°æå–æ‰€æœ‰é”šç‚¹æŒ‡çº¹")
    logger.info("="*80)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = project_root / "data" / "anchor_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ¢é’ˆ
    logger.info(f"\nğŸ“‹ åŠ è½½æ¢é’ˆ...")
    
    probes_path = project_root / "data" / "probes" / "all_probes.json"
    if probes_path.exists():
        logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æ¢é’ˆæ•°æ®: {probes_path}")
        with open(probes_path, 'r', encoding='utf-8') as f:
            probes_data = json.load(f)
    else:
        logger.info("æ„å»ºæ–°çš„æ¢é’ˆæ•°æ®é›†")
        probes_data = build_all_probes()
    
    # åˆå¹¶æ‰€æœ‰æ¢é’ˆ
    all_probes = []
    for probe_type, probes in probes_data.items():
        all_probes.extend(probes)
    
    # å¦‚æœæŒ‡å®šäº†æ¢é’ˆæ•°é‡ï¼ŒéšæœºæŠ½æ ·
    if args.num_probes > 0 and len(all_probes) > args.num_probes * 3:
        import random
        random.seed(42)
        all_probes = random.sample(all_probes, args.num_probes * 3)
        logger.info(f"å·²æŠ½æ · {len(all_probes)} ä¸ªæ¢é’ˆ")
    else:
        logger.info(f"ä½¿ç”¨å…¨éƒ¨ {len(all_probes)} ä¸ªæ¢é’ˆ")
    
    logger.success(f"âœ… å·²åŠ è½½ {len(all_probes)} ä¸ªæ¢é’ˆ")
    
    # é€‰æ‹©è¦æå–çš„é”šç‚¹
    if args.anchors:
        anchors_to_extract = {k: v for k, v in ANCHOR_MODEL_MAPPING.items() if k in args.anchors}
        if not anchors_to_extract:
            logger.error(f"âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„é”šç‚¹: {args.anchors}")
            sys.exit(1)
    else:
        anchors_to_extract = ANCHOR_MODEL_MAPPING
    
    logger.info(f"\nå°†æå– {len(anchors_to_extract)} ä¸ªé”šç‚¹:")
    for name in anchors_to_extract:
        logger.info(f"  â€¢ {name}")
    
    # æå–æ‰€æœ‰é”šç‚¹
    results = []
    for anchor_name, config in anchors_to_extract.items():
        result = extract_single_anchor(
            anchor_name=anchor_name,
            config=config,
            probes=all_probes,
            output_dir=output_dir,
            force=args.force
        )
        if result:
            results.append(result)
    
    # æ›´æ–° metadata
    logger.info(f"\nğŸ“ æ›´æ–° metadata.json...")
    metadata = {}
    
    for result in results:
        anchor_name = result["anchor_name"]
        config = result["config"]
        
        metadata[anchor_name] = {
            "metadata": config["metadata"],
            "fingerprint_file": result["fingerprint_file"],
            "has_fingerprint": True,
            "hf_model": config["hf_model"],
            "ollama_model": config["ollama_model"],
            "engine": config["engine"],
            "dimension": result["dimension"]
        }
    
    metadata_file = output_dir / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    logger.success(f"âœ… metadata å·²ä¿å­˜: {metadata_file}")
    
    # æ€»ç»“
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š æå–ç»“æœæ€»ç»“")
    logger.info(f"{'='*80}")
    logger.info(f"æˆåŠŸ: {len(results)}/{len(anchors_to_extract)}")
    
    if results:
        logger.info(f"\næˆåŠŸæå–çš„é”šç‚¹:")
        for result in results:
            logger.info(f"  âœ… {result['anchor_name']:20s} (ç»´åº¦: {result['dimension']})")
    
    failed = len(anchors_to_extract) - len(results)
    if failed > 0:
        logger.warning(f"\nâš ï¸  å¤±è´¥: {failed} ä¸ªé”šç‚¹")
    
    logger.success(f"\nğŸ‰ å®Œæˆï¼")


if __name__ == "__main__":
    main()
