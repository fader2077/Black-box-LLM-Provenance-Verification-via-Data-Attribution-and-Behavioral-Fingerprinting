"""
æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—çš„æœ‰æ•ˆæ€§

éªŒè¯ç­–ç•¥:
1. ä½¿ç”¨åŒä¸€æ¨¡å‹æå–ä¸¤æ¬¡æŒ‡çº¹ â†’ ç›¸ä¼¼åº¦åº”è¯¥å¾ˆé«˜ (>0.9)
2. ä½¿ç”¨ä¸åŒæ¨¡å‹æå–æŒ‡çº¹ â†’ ç›¸ä¼¼åº¦åº”è¯¥è¾ƒä½ (<0.5)
3. ç¡®è®¤æŒ‡çº¹æå–å’Œç›¸ä¼¼åº¦è®¡ç®—åŠŸèƒ½æ­£å¸¸
"""

import json
from pathlib import Path
from loguru import logger

from src.utils.unified_loader import load_model
from src.fingerprint import extract_fingerprint
from src.attribution.similarity import SimilarityCalculator
from src.probes import build_all_probes


def test_same_model_similarity():
    """æµ‹è¯• 1: åŒä¸€æ¨¡å‹çš„ä¸¤æ¬¡æŒ‡çº¹æå–åº”è¯¥é«˜åº¦ç›¸ä¼¼"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯• 1: åŒä¸€æ¨¡å‹ç›¸ä¼¼åº¦éªŒè¯ (GPT-2 vs GPT-2)")
    logger.info("=" * 80)
    
    # åŠ è½½æ¢é’ˆ
    probes_file = Path("data/probes/all_probes.json")
    if not probes_file.exists():
        logger.info("æ„å»ºæ¢é’ˆ...")
        build_all_probes()
    
    with open(probes_file, 'r', encoding='utf-8') as f:
        all_probes = json.load(f)
    
    # ä½¿ç”¨å°‘é‡æ¢é’ˆè¿›è¡Œæµ‹è¯•
    test_probes = []
    for probe_type in all_probes.keys():
        test_probes.extend(all_probes[probe_type][:10])  # æ¯ç±»å– 10 ä¸ª
    
    logger.info(f"ä½¿ç”¨ {len(test_probes)} ä¸ªæ¢é’ˆè¿›è¡Œæµ‹è¯•")
    
    # åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½ GPT-2 æ¨¡å‹...")
    model = load_model("gpt2", engine="transformers")
    
    # ç¬¬ä¸€æ¬¡æå–æŒ‡çº¹
    logger.info("ç¬¬ä¸€æ¬¡æå–æŒ‡çº¹...")
    fp1 = extract_fingerprint(model, test_probes)
    
    # ç¬¬äºŒæ¬¡æå–æŒ‡çº¹
    logger.info("ç¬¬äºŒæ¬¡æå–æŒ‡çº¹...")
    fp2 = extract_fingerprint(model, test_probes)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    calc = SimilarityCalculator()
    similarity = calc.calculate_fingerprint_similarity(fp1, fp2)
    
    logger.info("\n" + "=" * 80)
    logger.info("ç»“æœåˆ†æ:")
    logger.info("=" * 80)
    logger.info(f"æ¨¡å‹: GPT-2 vs GPT-2 (åŒä¸€æ¨¡å‹)")
    logger.info(f"æŒ‡çº¹ç»´åº¦: {fp1['logit_fingerprint']['dimension']}")
    
    if similarity.get("logit_similarity"):
        cosine = similarity["logit_similarity"].get("cosine_similarity", 0)
        logger.info(f"ä½™å¼¦ç›¸ä¼¼åº¦: {cosine:.4f}")
        logger.info(f"æ¬§æ°è·ç¦»: {similarity['logit_similarity'].get('euclidean_distance', 0):.4f}")
        logger.info(f"çš®å°”é€Šç›¸å…³: {similarity['logit_similarity'].get('pearson_correlation', 0):.4f}")
        
        # åˆ¤æ–­ç»“æœ
        if cosine > 0.9:
            logger.success(f"âœ… æµ‹è¯•é€šè¿‡: åŒä¸€æ¨¡å‹ç›¸ä¼¼åº¦ {cosine:.4f} > 0.9")
            return True
        else:
            logger.warning(f"âš ï¸ æµ‹è¯•å¼‚å¸¸: åŒä¸€æ¨¡å‹ç›¸ä¼¼åº¦ {cosine:.4f} åº”è¯¥ > 0.9")
            logger.warning("è¿™å¯èƒ½è¡¨æ˜æŒ‡çº¹æå–æœ‰éšæœºæ€§æˆ–ä¸ç¨³å®š")
            return False
    else:
        logger.error("âŒ æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦: logit_similarity ä¸ºç©º")
        return False


def test_different_models_similarity():
    """æµ‹è¯• 2: ä¸åŒæ¨¡å‹çš„æŒ‡çº¹åº”è¯¥ä¸ç›¸ä¼¼"""
    logger.info("\n" + "=" * 80)
    logger.info("æµ‹è¯• 2: ä¸åŒæ¨¡å‹ç›¸ä¼¼åº¦éªŒè¯ (GPT-2 vs GPT-2-Medium)")
    logger.info("=" * 80)
    
    # åŠ è½½æ¢é’ˆ
    probes_file = Path("data/probes/all_probes.json")
    with open(probes_file, 'r', encoding='utf-8') as f:
        all_probes = json.load(f)
    
    # ä½¿ç”¨å°‘é‡æ¢é’ˆ
    test_probes = []
    for probe_type in all_probes.keys():
        test_probes.extend(all_probes[probe_type][:10])
    
    logger.info(f"ä½¿ç”¨ {len(test_probes)} ä¸ªæ¢é’ˆè¿›è¡Œæµ‹è¯•")
    
    # åŠ è½½ç¬¬ä¸€ä¸ªæ¨¡å‹
    logger.info("åŠ è½½ GPT-2 æ¨¡å‹...")
    model1 = load_model("gpt2", engine="transformers")
    
    # åŠ è½½ç¬¬äºŒä¸ªæ¨¡å‹
    logger.info("åŠ è½½ GPT-2-Medium æ¨¡å‹...")
    model2 = load_model("gpt2-medium", engine="transformers")
    
    # æå–æŒ‡çº¹
    logger.info("æå– GPT-2 æŒ‡çº¹...")
    fp1 = extract_fingerprint(model1, test_probes)
    
    logger.info("æå– GPT-2-Medium æŒ‡çº¹...")
    fp2 = extract_fingerprint(model2, test_probes)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    calc = SimilarityCalculator()
    similarity = calc.calculate_fingerprint_similarity(fp1, fp2)
    
    logger.info("\n" + "=" * 80)
    logger.info("ç»“æœåˆ†æ:")
    logger.info("=" * 80)
    logger.info(f"æ¨¡å‹: GPT-2 vs GPT-2-Medium (ä¸åŒè§„æ¨¡)")
    logger.info(f"GPT-2 æŒ‡çº¹ç»´åº¦: {fp1['logit_fingerprint']['dimension']}")
    logger.info(f"GPT-2-Medium æŒ‡çº¹ç»´åº¦: {fp2['logit_fingerprint']['dimension']}")
    
    if similarity.get("logit_similarity"):
        cosine = similarity["logit_similarity"].get("cosine_similarity", 0)
        logger.info(f"ä½™å¼¦ç›¸ä¼¼åº¦: {cosine:.4f}")
        logger.info(f"æ¬§æ°è·ç¦»: {similarity['logit_similarity'].get('euclidean_distance', 0):.4f}")
        logger.info(f"çš®å°”é€Šç›¸å…³: {similarity['logit_similarity'].get('pearson_correlation', 0):.4f}")
        
        # åˆ¤æ–­ç»“æœ
        if 0.3 < cosine < 0.8:
            logger.success(f"âœ… æµ‹è¯•é€šè¿‡: ä¸åŒæ¨¡å‹ç›¸ä¼¼åº¦ {cosine:.4f} åœ¨åˆç†èŒƒå›´ (0.3-0.8)")
            logger.info("GPT-2 å’Œ GPT-2-Medium æ˜¯åŒç³»åˆ—æ¨¡å‹ï¼Œæœ‰ä¸€å®šç›¸ä¼¼æ€§æ˜¯æ­£å¸¸çš„")
            return True
        elif cosine < 0.3:
            logger.warning(f"âš ï¸ ç›¸ä¼¼åº¦ {cosine:.4f} è¾ƒä½ï¼Œä½†ä¹Ÿå¯èƒ½æ­£å¸¸")
            return True
        else:
            logger.warning(f"âš ï¸ ç›¸ä¼¼åº¦ {cosine:.4f} è¾ƒé«˜ï¼Œå¯èƒ½æŒ‡çº¹åŒºåˆ†åº¦ä¸å¤Ÿ")
            return False
    else:
        logger.error("âŒ æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦: logit_similarity ä¸ºç©º")
        return False


def test_anchor_fingerprint_extraction():
    """æµ‹è¯• 3: ä½¿ç”¨ Transformers æå–é”šç‚¹æŒ‡çº¹"""
    logger.info("\n" + "=" * 80)
    logger.info("æµ‹è¯• 3: ä½¿ç”¨ Transformers å¼•æ“æå–å¯ç”¨çš„é”šç‚¹æŒ‡çº¹")
    logger.info("=" * 80)
    
    # ä½¿ç”¨ HuggingFace ä¸Šå¯ç”¨çš„æ¨¡å‹ä½œä¸ºé”šç‚¹
    anchor_models = [
        ("gpt2", "GPT-2 (OpenAI)"),
        ("google/gemma-2-2b-it", "Gemma-2-2B (Google)"),
        ("Qwen/Qwen2.5-0.5B", "Qwen2.5-0.5B (Alibaba)"),
    ]
    
    # åŠ è½½æ¢é’ˆ
    probes_file = Path("data/probes/all_probes.json")
    with open(probes_file, 'r', encoding='utf-8') as f:
        all_probes = json.load(f)
    
    # ä½¿ç”¨ 30 ä¸ªæ¢é’ˆ
    test_probes = []
    for probe_type in all_probes.keys():
        test_probes.extend(all_probes[probe_type][:10])
    
    logger.info(f"ä½¿ç”¨ {len(test_probes)} ä¸ªæ¢é’ˆ")
    
    # åˆ›å»ºä¸´æ—¶é”šç‚¹æ•°æ®åº“
    anchor_dir = Path("data/anchor_models_transformers")
    anchor_dir.mkdir(exist_ok=True)
    
    anchor_fingerprints = {}
    
    for model_name, description in anchor_models:
        try:
            logger.info(f"\nå¤„ç†: {description} ({model_name})")
            
            # åŠ è½½æ¨¡å‹
            logger.info("  è½½å…¥æ¨¡å‹...")
            model = load_model(model_name, engine="transformers")
            
            # æå–æŒ‡çº¹
            logger.info("  æå–æŒ‡çº¹...")
            fingerprint = extract_fingerprint(model, test_probes)
            
            # ä¿å­˜æŒ‡çº¹
            safe_name = model_name.replace("/", "_").replace(":", "_")
            fp_file = anchor_dir / f"{safe_name}_fingerprint.json"
            
            with open(fp_file, 'w', encoding='utf-8') as f:
                json.dump(fingerprint, f, indent=2, ensure_ascii=False)
            
            anchor_fingerprints[model_name] = fingerprint
            
            logger.success(f"  âœ“ æŒ‡çº¹å·²ä¿å­˜: {fp_file.name}")
            logger.info(f"  ç»´åº¦: {fingerprint['logit_fingerprint']['dimension']}")
            
        except Exception as e:
            logger.error(f"  âœ— å¤±è´¥: {e}")
    
    # è®¡ç®—äº¤å‰ç›¸ä¼¼åº¦
    if len(anchor_fingerprints) >= 2:
        logger.info("\n" + "=" * 80)
        logger.info("äº¤å‰ç›¸ä¼¼åº¦çŸ©é˜µ:")
        logger.info("=" * 80)
        
        calc = SimilarityCalculator()
        model_names = list(anchor_fingerprints.keys())
        
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i < j:
                    fp1 = anchor_fingerprints[model1]
                    fp2 = anchor_fingerprints[model2]
                    
                    similarity = calc.calculate_fingerprint_similarity(fp1, fp2)
                    cosine = similarity.get("logit_similarity", {}).get("cosine_similarity", 0)
                    
                    logger.info(f"{model1} <-> {model2}: {cosine:.4f}")
        
        logger.success("\nâœ… é”šç‚¹æŒ‡çº¹æå–å®Œæˆ")
        return True
    else:
        logger.warning("âš ï¸ å¯ç”¨é”šç‚¹ä¸è¶³")
        return False


def main():
    logger.info("=" * 80)
    logger.info("ç›¸ä¼¼åº¦è®¡ç®—éªŒè¯æµ‹è¯•å¥—ä»¶")
    logger.info("=" * 80)
    
    results = []
    
    # æµ‹è¯• 1: åŒä¸€æ¨¡å‹
    try:
        result1 = test_same_model_similarity()
        results.append(("åŒä¸€æ¨¡å‹ç›¸ä¼¼åº¦", result1))
    except Exception as e:
        logger.error(f"æµ‹è¯• 1 å¤±è´¥: {e}")
        results.append(("åŒä¸€æ¨¡å‹ç›¸ä¼¼åº¦", False))
    
    # æµ‹è¯• 2: ä¸åŒæ¨¡å‹
    try:
        result2 = test_different_models_similarity()
        results.append(("ä¸åŒæ¨¡å‹ç›¸ä¼¼åº¦", result2))
    except Exception as e:
        logger.error(f"æµ‹è¯• 2 å¤±è´¥: {e}")
        results.append(("ä¸åŒæ¨¡å‹ç›¸ä¼¼åº¦", False))
    
    # æµ‹è¯• 3: é”šç‚¹æå–
    try:
        result3 = test_anchor_fingerprint_extraction()
        results.append(("é”šç‚¹æŒ‡çº¹æå–", result3))
    except Exception as e:
        logger.error(f"æµ‹è¯• 3 å¤±è´¥: {e}")
        results.append(("é”šç‚¹æŒ‡çº¹æå–", False))
    
    # æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info("=" * 80)
    
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        logger.info(f"{test_name:20s}: {status}")
    
    total = len(results)
    passed = sum(1 for _, p in results if p)
    
    logger.info(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        logger.success("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç›¸ä¼¼åº¦è®¡ç®—åŠŸèƒ½æ­£å¸¸ã€‚")
    else:
        logger.warning(f"\nâš ï¸ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚")


if __name__ == "__main__":
    main()
