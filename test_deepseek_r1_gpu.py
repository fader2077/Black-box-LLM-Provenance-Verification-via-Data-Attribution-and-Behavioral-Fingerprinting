# -*- coding: utf-8 -*-
"""
DeepSeek-R1-Distill-Llama-8B GPUæµ‹è¯•
å¼ºåˆ¶ä½¿ç”¨GPUè¿›è¡Œè¯„ä¼°
"""
import sys
import json
import torch
from pathlib import Path
from loguru import logger

# é…ç½® logger
logger.remove()
logger.add(sys.stderr, level="INFO")
logger.add("deepseek_r1_gpu_test.log", level="DEBUG", encoding="utf-8")

def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    logger.info("="*80)
    logger.info("æ£€æŸ¥GPUçŠ¶æ€")
    logger.info("="*80)
    
    if not torch.cuda.is_available():
        logger.error("âŒ CUDAä¸å¯ç”¨ï¼æ­¤æµ‹è¯•éœ€è¦GPUã€‚")
        logger.error("è¯·ç¡®ä¿ï¼š")
        logger.error("  1. å®‰è£…äº†æ”¯æŒCUDAçš„PyTorch")
        logger.error("  2. ç³»ç»Ÿæœ‰å¯ç”¨çš„NVIDIA GPU")
        logger.error("  3. å·²å®‰è£…CUDAé©±åŠ¨")
        return False
    
    logger.success(f"âœ“ CUDAå¯ç”¨")
    logger.info(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
    logger.info(f"  å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    logger.info(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")
    logger.info(f"  æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    return True

def main():
    # æ£€æŸ¥GPU
    if not check_gpu():
        return 1
    
    logger.info("\n" + "="*80)
    logger.info("DeepSeek-R1-Distill-Llama-8B è¯„ä¼°ï¼ˆGPUåŠ é€Ÿï¼‰")
    logger.info("="*80)
    
    from src.fingerprint import extract_fingerprint
    from src.attribution.similarity import SimilarityCalculator
    from src.attribution.anchor_models import AnchorModelsDatabase
    
    # å¼ºåˆ¶ä½¿ç”¨transformers + GPU
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    # åŠ¨æ€å¯¼å…¥ä»¥ç¡®ä¿ä½¿ç”¨GPU
    from src.utils.unified_loader import load_model
    
    # åŠ è½½æ¢é’ˆ
    logger.info("\n[1/5] åŠ è½½æ¢é’ˆæ•°æ®...")
    probe_file = Path("data/probes/all_probes.json")
    with open(probe_file, 'r', encoding='utf-8') as f:
        probes_data = json.load(f)
    
    probes = []
    for category, items in probes_data.items():
        if isinstance(items, list):
            probes.extend(items)
    
    logger.success(f"âœ“ å·²åŠ è½½ {len(probes)} ä¸ªæ¢é’ˆ")
    
    # åŠ è½½æ¨¡å‹ï¼ˆGPUï¼‰
    logger.info("\n[2/5] åŠ è½½ DeepSeek-R1-Distill-Llama-8B åˆ° GPU...")
    model = load_model("deepseek-ai/DeepSeek-R1-Distill-Llama-8B", engine="transformers")
    
    # éªŒè¯æ¨¡å‹åœ¨GPUä¸Š
    if hasattr(model, 'loader') and hasattr(model.loader, 'device'):
        device = model.loader.device
        logger.success(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ°: {device}")
        
        if device == "cpu":
            logger.error("âŒ æ¨¡å‹åœ¨CPUä¸Šï¼éœ€è¦GPUã€‚")
            return 1
    
    # æå–æŒ‡çº¹
    logger.info(f"\n[3/5] æå–æŒ‡çº¹ï¼ˆä½¿ç”¨ {len(probes)} ä¸ªæ¢é’ˆï¼‰...")
    logger.info("æ­¤è¿‡ç¨‹åœ¨GPUä¸Šä¼šå¿«å¾ˆå¤šï¼Œè¯·ç¨å€™...")
    
    try:
        fingerprint = extract_fingerprint(model, probes=probes)
        logger.success(f"âœ“ æŒ‡çº¹æå–æˆåŠŸ")
        logger.info(f"  å‘é‡ç»´åº¦: {len(fingerprint['logit_fingerprint']['vector'])}")
    except Exception as e:
        logger.error(f"âŒ æŒ‡çº¹æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # åŠ è½½é”šç‚¹æ¨¡å‹
    logger.info("\n[4/5] åŠ è½½é”šç‚¹æ¨¡å‹å¹¶è®¡ç®—ç›¸ä¼¼åº¦...")
    db = AnchorModelsDatabase()
    anchors = db.list_all_anchors()
    
    logger.info(f"å¯ç”¨é”šç‚¹: {len(anchors)} ä¸ª")
    for name, info in anchors.items():
        has_fp = "âœ“" if info['has_fingerprint'] else "âœ—"
        logger.info(f"  {has_fp} {name}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    calculator = SimilarityCalculator()
    logger.info("\nè®¡ç®—ç›¸ä¼¼åº¦...")
    
    results = {}
    for anchor_name, anchor_info in anchors.items():
        if anchor_info['has_fingerprint']:
            anchor_fp = db.load_fingerprint(anchor_name)
            if anchor_fp:
                similarity = calculator.calculate_fingerprint_similarity(fingerprint, anchor_fp)
                results[anchor_name] = similarity['overall_similarity']
                logger.info(f"  {anchor_name:30s}: {similarity['overall_similarity']:.4f}")
    
    # åˆ†æç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ç›¸ä¼¼åº¦åˆ†æç»“æœ")
    logger.info("="*80)
    
    if not results:
        logger.error("âŒ æ²¡æœ‰è®¡ç®—å‡ºç›¸ä¼¼åº¦ç»“æœ")
        return 1
    
    # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„
    best_match = max(results.items(), key=lambda x: x[1])
    logger.success(f"\næœ€ç›¸ä¼¼çš„é”šç‚¹: {best_match[0]}")
    logger.success(f"ç›¸ä¼¼åº¦: {best_match[1]:.4f} ({best_match[1]*100:.2f}%)")
    
    # åˆ†ç±»ç»Ÿè®¡
    deepseek_scores = {k: v for k, v in results.items() if 'deepseek' in k.lower()}
    llama_scores = {k: v for k, v in results.items() if 'llama' in k.lower()}
    gpt_scores = {k: v for k, v in results.items() if 'gpt' in k.lower()}
    
    logger.info("\nåˆ†ç±»ç»Ÿè®¡:")
    
    if deepseek_scores:
        avg_deepseek = sum(deepseek_scores.values()) / len(deepseek_scores)
        logger.info(f"\nDeepSeek ç³»åˆ—:")
        for model, score in deepseek_scores.items():
            logger.info(f"  {model:30s}: {score:.4f}")
        logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_deepseek:.4f}")
    
    if llama_scores:
        avg_llama = sum(llama_scores.values()) / len(llama_scores)
        logger.info(f"\nLlama ç³»åˆ—:")
        for model, score in llama_scores.items():
            logger.info(f"  {model:30s}: {score:.4f}")
        logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_llama:.4f}")
    
    if gpt_scores:
        avg_gpt = sum(gpt_scores.values()) / len(gpt_scores)
        logger.info(f"\nGPT ç³»åˆ—:")
        for model, score in gpt_scores.items():
            logger.info(f"  {model:30s}: {score:.4f}")
        logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_gpt:.4f}")
    
    # ç»“è®º
    logger.info("\n" + "="*80)
    logger.info("ç»“è®º")
    logger.info("="*80)
    
    if deepseek_scores and llama_scores:
        if avg_deepseek > avg_llama:
            diff = avg_deepseek - avg_llama
            logger.success(f"âœ“ DeepSeek-R1-Distill-Llama-8B æ›´æ¥è¿‘ DeepSeek ç³»åˆ—")
            logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦å·®å¼‚: {diff:.4f} ({diff*100:.2f}%)")
        else:
            diff = avg_llama - avg_deepseek
            logger.success(f"âœ“ DeepSeek-R1-Distill-Llama-8B æ›´æ¥è¿‘ Llama ç³»åˆ—")
            logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦å·®å¼‚: {diff:.4f} ({diff*100:.2f}%)")
    elif deepseek_scores:
        logger.info(f"DeepSeek-R1-Distill-Llama-8B ä¸ DeepSeek ç³»åˆ—çš„å¹³å‡ç›¸ä¼¼åº¦: {avg_deepseek:.4f}")
    elif llama_scores:
        logger.info(f"DeepSeek-R1-Distill-Llama-8B ä¸ Llama ç³»åˆ—çš„å¹³å‡ç›¸ä¼¼åº¦: {avg_llama:.4f}")
    else:
        logger.warning("âš  ç¼ºå°‘ Llama æˆ– DeepSeek é”šç‚¹ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")
    
    # ä¿å­˜ç»“æœ
    logger.info("\n[5/5] ä¿å­˜ç»“æœ...")
    result_file = Path("results/deepseek_r1_distill_llama_8b_evaluation.json")
    result_file.parent.mkdir(parents=True, exist_ok=True)
    
    result_data = {
        "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "device": "cuda",
        "probe_count": len(probes),
        "similarity_scores": results,
        "best_match": {
            "model": best_match[0],
            "score": best_match[1]
        },
        "category_averages": {
            "deepseek": avg_deepseek if deepseek_scores else None,
            "llama": avg_llama if llama_scores else None,
            "gpt": avg_gpt if gpt_scores else None,
        }
    }
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, indent=2, ensure_ascii=False)
    
    logger.success(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    logger.info("\n" + "="*80)
    logger.success("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    logger.info("="*80)
    
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.warning("\nâš  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
