"""
ä½¿ç”¨ Transformers å¼•æ“é‡æ–°æå–é”šç‚¹æ¨¡å‹æŒ‡çº¹

ç”±äº Ollama 0.14.1 ä¸æ”¯æŒ logprobsï¼Œæˆ‘ä»¬ä½¿ç”¨ HuggingFace ä¸Šçš„å¯¹åº”æ¨¡å‹é‡æ–°æå–
"""

import json
from pathlib import Path
from loguru import logger

from src.utils.unified_loader import load_model
from src.fingerprint import extract_fingerprint
from src.probes import build_all_probes


# HuggingFace æ¨¡å‹æ˜ å°„ï¼ˆå¯¹åº”åŸæœ‰çš„ Ollama æ¨¡å‹ï¼‰
ANCHOR_MODEL_MAPPING = {
    # åŸ Ollama æ¨¡å‹å -> (HuggingFace æ¨¡å‹, æè¿°, å…ƒæ•°æ®)
    "qwen2.5:7b": {
        "hf_model": "Qwen/Qwen2.5-0.5B",  # ä½¿ç”¨è¾ƒå°çš„ç‰ˆæœ¬ä»¥èŠ‚çœæ—¶é—´/èµ„æº
        "description": "é˜¿é‡Œå·´å·´ Qwen ç³»åˆ—æ¨¡å‹",
        "metadata": {
            "name": "qwen2.5:0.5b",
            "source": "china",
            "category": "qwen",
            "vendor": "Alibaba",
            "base_model": "Qwen2.5-0.5B",
            "description": "é˜¿é‡Œå·´å·´ Qwen ç³»åˆ—æ¨¡å‹ï¼ˆ0.5Bå‚æ•°ï¼‰"
        }
    },
    "deepseek-r1:7b": {
        "hf_model": "gpt2",  # DeepSeek R1 éœ€è¦æˆæƒï¼Œä½¿ç”¨ GPT-2 ä½œä¸ºæ›¿ä»£ç¤ºä¾‹
        "description": "OpenAI GPT-2ï¼ˆæ›¿ä»£ DeepSeek R1ï¼‰",
        "metadata": {
            "name": "gpt2",
            "source": "openai",
            "category": "gpt",
            "vendor": "OpenAI",
            "base_model": "GPT-2",
            "description": "OpenAI GPT-2 æ¨¡å‹"
        }
    },
    "yi:6b": {
        "hf_model": "01-ai/Yi-6B",  # Yi æ¨¡å‹å¯èƒ½éœ€è¦æˆæƒ
        "description": "é›¶ä¸€ä¸‡ç‰© Yi ç³»åˆ—",
        "metadata": {
            "name": "yi:6b",
            "source": "china",
            "category": "yi",
            "vendor": "01.AI",
            "base_model": "Yi-6B",
            "description": "é›¶ä¸€ä¸‡ç‰© Yi ç³»åˆ—æ¨¡å‹"
        }
    },
    "llama3.2:3b": {
        "hf_model": "meta-llama/Llama-3.2-1B",  # Llama 3.2 éœ€è¦æˆæƒï¼Œä½¿ç”¨1Bç‰ˆæœ¬
        "description": "Meta Llama 3.2 ç³»åˆ—",
        "metadata": {
            "name": "llama3.2:1b",
            "source": "meta",
            "category": "llama",
            "vendor": "Meta",
            "base_model": "Llama-3.2-1B",
            "description": "Meta Llama 3.2 ç³»åˆ—ï¼ˆ1Bå‚æ•°ï¼‰"
        }
    },
    "gemma2:2b": {
        "hf_model": "google/gemma-2b",  # Gemma 2B
        "description": "Google Gemma 2B",
        "metadata": {
            "name": "gemma:2b",
            "source": "google",
            "category": "gemma",
            "vendor": "Google",
            "base_model": "Gemma-2B",
            "description": "Google Gemma ç³»åˆ—ï¼ˆ2Bå‚æ•°ï¼‰"
        }
    }
}


def extract_anchor_fingerprints(num_probes: int = 50):
    """
    ä½¿ç”¨ Transformers å¼•æ“æå–é”šç‚¹æ¨¡å‹æŒ‡çº¹
    
    Args:
        num_probes: æ¯ç±»æ¢é’ˆä½¿ç”¨çš„æ•°é‡
    """
    logger.info("=" * 80)
    logger.info("ä½¿ç”¨ Transformers å¼•æ“é‡æ–°æå–é”šç‚¹æ¨¡å‹æŒ‡çº¹")
    logger.info("=" * 80)
    
    # 1. åŠ è½½æ¢é’ˆ
    probes_file = Path("data/probes/all_probes.json")
    if not probes_file.exists():
        logger.info("æ„å»ºæ¢é’ˆ...")
        build_all_probes()
    
    with open(probes_file, 'r', encoding='utf-8') as f:
        all_probes = json.load(f)
    
    # ä½¿ç”¨æ‰€æœ‰æ¢é’ˆï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
    test_probes = []
    for probe_type in all_probes.keys():
        test_probes.extend(all_probes[probe_type])  # ğŸ”§ ç§»é™¤æ•°é‡é™åˆ¶ï¼Œä½¿ç”¨å…¨éƒ¨æ¢é’ˆ
    
    logger.info(f"ä½¿ç”¨ {len(test_probes)} ä¸ªæ¢é’ˆè¿›è¡ŒæŒ‡çº¹æå– (å®Œæ•´æ•°æ®é›†)")
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    anchor_dir = Path("data/anchor_models")
    anchor_dir.mkdir(exist_ok=True, parents=True)
    
    # 3. æå–æ¯ä¸ªé”šç‚¹æ¨¡å‹çš„æŒ‡çº¹
    metadata_dict = {}
    success_count = 0
    
    for ollama_name, config in ANCHOR_MODEL_MAPPING.items():
        hf_model = config["hf_model"]
        description = config["description"]
        metadata = config["metadata"]
        
        logger.info(f"\n{'=' * 80}")
        logger.info(f"å¤„ç†é”šç‚¹: {description}")
        logger.info(f"  åŸæ¨¡å‹: {ollama_name}")
        logger.info(f"  HFæ¨¡å‹: {hf_model}")
        logger.info(f"{'=' * 80}")
        
        try:
            # åŠ è½½æ¨¡å‹
            logger.info("  [1/3] åŠ è½½æ¨¡å‹...")
            model = load_model(hf_model, engine="transformers")
            
            # æå–æŒ‡çº¹
            logger.info("  [2/3] æå–æŒ‡çº¹...")
            fingerprint = extract_fingerprint(model, test_probes)
            
            # ä¿å­˜æŒ‡çº¹
            logger.info("  [3/3] ä¿å­˜æŒ‡çº¹...")
            safe_name = ollama_name.replace(":", "_").replace("/", "_")
            fp_file = anchor_dir / f"{safe_name}_fingerprint.json"
            
            with open(fp_file, 'w', encoding='utf-8') as f:
                json.dump(fingerprint, f, indent=2, ensure_ascii=False)
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata_dict[metadata["name"]] = {
                "metadata": metadata,
                "fingerprint_file": str(fp_file),
                "has_fingerprint": True,
                "hf_model": hf_model
            }
            
            logger.success(f"  âœ“ æŒ‡çº¹å·²ä¿å­˜: {fp_file.name}")
            logger.info(f"  ç»´åº¦: {fingerprint['logit_fingerprint']['dimension']}")
            success_count += 1
            
        except Exception as e:
            logger.error(f"  âœ— å¤±è´¥: {e}")
            logger.warning(f"  è·³è¿‡æ¨¡å‹: {hf_model}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿè¦è®°å½•å…ƒæ•°æ®
            metadata_dict[metadata["name"]] = {
                "metadata": metadata,
                "fingerprint_file": f"data/anchor_models/{safe_name}_fingerprint.json",
                "has_fingerprint": False,
                "hf_model": hf_model,
                "error": str(e)
            }
    
    # 4. ä¿å­˜å…ƒæ•°æ®
    logger.info(f"\n{'=' * 80}")
    logger.info("ä¿å­˜é”šç‚¹æ•°æ®åº“å…ƒæ•°æ®...")
    metadata_file = anchor_dir / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata_dict, f, indent=2, ensure_ascii=False)
    
    logger.success(f"âœ“ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    
    # 5. æ€»ç»“
    logger.info(f"\n{'=' * 80}")
    logger.info("æå–æ€»ç»“")
    logger.info(f"{'=' * 80}")
    logger.info(f"æˆåŠŸ: {success_count}/{len(ANCHOR_MODEL_MAPPING)}")
    logger.info(f"å¤±è´¥: {len(ANCHOR_MODEL_MAPPING) - success_count}/{len(ANCHOR_MODEL_MAPPING)}")
    
    if success_count == len(ANCHOR_MODEL_MAPPING):
        logger.success("\nğŸ‰ æ‰€æœ‰é”šç‚¹æŒ‡çº¹æå–æˆåŠŸï¼")
    elif success_count > 0:
        logger.warning(f"\nâš ï¸ éƒ¨åˆ†é”šç‚¹æå–æˆåŠŸ ({success_count}/{len(ANCHOR_MODEL_MAPPING)})")
    else:
        logger.error("\nâŒ æ‰€æœ‰é”šç‚¹æå–å¤±è´¥")
    
    return success_count > 0


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Transformers å¼•æ“é‡æ–°æå–é”šç‚¹æŒ‡çº¹")
    parser.add_argument("--num-probes", type=int, default=50,
                        help="æ¯ç±»æ¢é’ˆä½¿ç”¨çš„æ•°é‡ (é»˜è®¤: 50)")
    
    args = parser.parse_args()
    
    success = extract_anchor_fingerprints(args.num_probes)
    
    if success:
        logger.info("\n" + "=" * 80)
        logger.info("ä¸‹ä¸€æ­¥: é‡æ–°è¿è¡Œå®Œæ•´è¯„ä¼°")
        logger.info("=" * 80)
        logger.info("python experiments/full_evaluation.py --target-model gpt2 --engine transformers")
    else:
        logger.error("\né”šç‚¹æŒ‡çº¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
