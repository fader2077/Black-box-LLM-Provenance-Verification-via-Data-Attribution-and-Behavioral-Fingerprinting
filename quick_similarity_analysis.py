"""
åŸºäºç°æœ‰é”šç‚¹çš„DeepSeek-R1å¿«é€Ÿè¯„ä¼°
ä½¿ç”¨å·²æå–çš„é”šç‚¹æŒ‡çº¹è¿›è¡Œç›¸ä¼¼åº¦åˆ†æ
"""

import sys
import json
from pathlib import Path
from loguru import logger

sys.path.insert(0, str(Path(__file__).parent))

from src.attribution.similarity import SimilarityCalculator


def quick_analysis():
    """å¿«é€Ÿåˆ†æç°æœ‰æŒ‡çº¹"""
    
    logger.info("=" * 80)
    logger.info("DeepSeek-R1 å¿«é€Ÿç›¸ä¼¼åº¦åˆ†æ")
    logger.info("åŸºäºç°æœ‰é”šç‚¹æŒ‡çº¹")
    logger.info("=" * 80)
    
    # åŠ è½½é”šç‚¹æŒ‡çº¹
    anchors = [
        {
            "name": "gpt2",
            "path": "data/anchor_models/gpt2_fingerprint.json",
            "category": "gpt",
            "source": "openai"
        },
        {
            "name": "gpt2-medium",
            "path": "data/anchor_models/gpt2_medium_fingerprint.json",
            "category": "gpt",
            "source": "openai"
        },
        {
            "name": "deepseek-r1:7b",
            "path": "data/anchor_models/deepseek_r1_7b_fingerprint.json",
            "category": "deepseek",
            "source": "china"
        }
    ]
    
    logger.info(f"\nåŠ è½½ {len(anchors)} ä¸ªé”šç‚¹æŒ‡çº¹...")
    
    loaded_anchors = []
    for anchor in anchors:
        path = Path(anchor["path"])
        if path.exists():
            with open(path, 'r', encoding='utf-8') as f:
                fp = json.load(f)
                loaded_anchors.append({
                    **anchor,
                    "fingerprint": fp
                })
                logger.info(f"  âœ“ {anchor['name']}")
        else:
            logger.warning(f"  âœ— {anchor['name']} - æ–‡ä»¶ä¸å­˜åœ¨")
    
    if len(loaded_anchors) < 2:
        logger.error("é”šç‚¹æŒ‡çº¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæ¯”è¾ƒ")
        return
    
    # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†æå–çš„DeepSeek-R1æŒ‡çº¹
    deepseek_fps = list(Path("checkpoints").glob("deepseek*checkpoint.json"))
    deepseek_fps.extend(list(Path("results").glob("deepseek*fingerprint.json")))
    
    if not deepseek_fps:
        logger.error("æœªæ‰¾åˆ°DeepSeek-R1æŒ‡çº¹æ–‡ä»¶")
        logger.info("\nå»ºè®®: å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æå–æŒ‡çº¹:")
        logger.info("  python experiments/robust_fingerprint_extraction.py --model deepseek-r1:8b --engine ollama --num-probes 50 --batch-size 5 --device cuda")
        return
    
    logger.info(f"\næ‰¾åˆ° {len(deepseek_fps)} ä¸ªDeepSeekç›¸å…³æ–‡ä»¶")
    for fp in deepseek_fps:
        logger.info(f"  - {fp}")
    
    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    latest_fp = max(deepseek_fps, key=lambda p: p.stat().st_mtime)
    logger.info(f"\nä½¿ç”¨æœ€æ–°æ–‡ä»¶: {latest_fp.name}")
    
    with open(latest_fp, 'r', encoding='utf-8') as f:
        target_data = json.load(f)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ£€æŸ¥ç‚¹æ–‡ä»¶
    if "partial_results" in target_data:
        logger.info("è¿™æ˜¯æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œéœ€è¦è½¬æ¢ä¸ºå®Œæ•´æŒ‡çº¹")
        # ç®€å•è½¬æ¢
        import numpy as np
        partial_results = target_data["partial_results"]
        
        # æå–ç‰¹å¾
        feature_vectors = []
        for result in partial_results:
            if 'error' not in result:
                feature_vectors.append(result['features'])
        
        all_features = np.concatenate(feature_vectors)
        
        target_fp = {
            "model_name": "deepseek-r1:8b (éƒ¨åˆ†)",
            "logit_fingerprint": {
                "vector": all_features.tolist(),
                "dimension": len(all_features)
            }
        }
        logger.info(f"  âœ“ è½¬æ¢æˆåŠŸ: {len(partial_results)} ä¸ªæ¢é’ˆ")
    else:
        target_fp = target_data
        logger.info(f"  âœ“ å®Œæ•´æŒ‡çº¹æ–‡ä»¶")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    logger.info("\nè®¡ç®—ç›¸ä¼¼åº¦...")
    sim_calc = SimilarityCalculator()
    
    results = []
    for anchor in loaded_anchors:
        anchor_fp = anchor["fingerprint"]
        
        sim_result = sim_calc.calculate_fingerprint_similarity(target_fp, anchor_fp)
        score = sim_result["overall_similarity"]
        
        results.append({
            "anchor": anchor["name"],
            "category": anchor["category"],
            "source": anchor["source"],
            "similarity": score
        })
        
        logger.info(f"  vs {anchor['name']:20s} [{anchor['category']:10s}] {score:.4f}")
    
    # æ’åºå¹¶è¾“å‡ºç»“è®º
    results.sort(key=lambda x: x["similarity"], reverse=True)
    
    logger.info("\n" + "=" * 80)
    logger.info("ç›¸ä¼¼åº¦æ’å")
    logger.info("=" * 80)
    
    for idx, result in enumerate(results, 1):
        logger.info(f"{idx}. {result['anchor']:20s} [{result['category']:10s}] {result['similarity']:.4f}")
    
    top_match = results[0]
    logger.info("\n" + "=" * 80)
    logger.info(f"âœ… æœ€ç›¸ä¼¼: {top_match['anchor']} ({top_match['similarity']:.4f})")
    logger.info(f"   ç±»åˆ«: {top_match['category']}")
    logger.info(f"   æ¥æº: {top_match['source']}")
    
    # ç±»åˆ«ç»Ÿè®¡
    category_scores = {}
    for result in results:
        cat = result['category']
        if cat not in category_scores:
            category_scores[cat] = []
        category_scores[cat].append(result['similarity'])
    
    category_avg = {cat: sum(scores)/len(scores) for cat, scores in category_scores.items()}
    
    logger.info("\nğŸ“Š ç±»åˆ«å¹³å‡ç›¸ä¼¼åº¦:")
    for cat, avg_score in sorted(category_avg.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"   {cat:15s} {avg_score:.4f}")
    
    logger.info("=" * 80)
    
    # ä¿å­˜ç»“æœ
    report = {
        "target_model": target_fp.get("model_name", "deepseek-r1:8b"),
        "source_file": str(latest_fp),
        "anchors_used": [a["name"] for a in loaded_anchors],
        "similarities": results,
        "category_averages": category_avg,
        "top_match": top_match
    }
    
    report_path = Path("results/quick_analysis_result.json")
    report_path.parent.mkdir(parents=True, exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nâœ“ æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")


if __name__ == "__main__":
    quick_analysis()
